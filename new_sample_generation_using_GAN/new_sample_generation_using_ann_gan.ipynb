{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tCINEY7Bc3lq"
   },
   "outputs": [],
   "source": [
    "# libraries\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing \n",
    "import numpy as np\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn import preprocessing\n",
    "scaler = preprocessing.StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "KSpiMkH1c3l_"
   },
   "outputs": [],
   "source": [
    "### for sorting based on number of samples\n",
    "def sortingFunction(data):\n",
    "    return data.shape[0]\n",
    "\n",
    "\n",
    "#### label coding for nominal values\n",
    "def label_coding(label):\n",
    "    dataset[label]= label_encoder.fit_transform(dataset[label]) \n",
    "    dataset[label].unique()\n",
    "    \n",
    "### \n",
    "def crop_dataset(len_dataset):\n",
    "    for label in range(len_dataset):\n",
    "        temp_dataframe=dataset[dataset['label']==label]\n",
    "        print(temp_dataframe.shape)\n",
    "        try:\n",
    "            _ ,temp_dataframe_ = train_test_split(temp_dataframe,test_size =0.50)\n",
    "           \n",
    "            temp_train ,temp_test = train_test_split(temp_dataframe_,test_size=0.25)\n",
    "            print(temp_train.shape,temp_test.shape)\n",
    "        except:\n",
    "            print(\"Error for \"+str(label))\n",
    "        list_train.append(temp_train)\n",
    "        list_test.append(temp_test)\n",
    "    \n",
    "        \n",
    "def new_sample_generation(x,y,z):\n",
    "    need=500-x if x<=500 else 0\n",
    "    print(\"n_sample: \"+str(x)+\" max_sample: \"+str(y)+\" need_to_create: \"+str(need))\n",
    "        \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WJ10Wyk-c3mI"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(494020, 42)"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "### importing dataset\n",
    "dataset = pd.read_csv('kddcup99_csv.csv')\n",
    "dataset.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 5858,
     "status": "ok",
     "timestamp": 1575153528382,
     "user": {
      "displayName": "Nur Imtiazul Haque",
      "photoUrl": "",
      "userId": "15560043854638298091"
     },
     "user_tz": 300
    },
    "id": "6XOAu1Thc3mR",
    "outputId": "2b1d9d6c-ebb2-43f8-a993-11377562eba7"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(2203, 21)\n",
      "(826, 21) (276, 21)\n",
      "(30, 21)\n",
      "(11, 21) (4, 21)\n",
      "(8, 21)\n",
      "(3, 21) (1, 21)\n",
      "(53, 21)\n",
      "(20, 21) (7, 21)\n",
      "(12, 21)\n",
      "(4, 21) (2, 21)\n",
      "(1247, 21)\n",
      "(468, 21) (156, 21)\n",
      "(21, 21)\n",
      "(8, 21) (3, 21)\n",
      "(9, 21)\n",
      "(3, 21) (2, 21)\n",
      "(7, 21)\n",
      "(3, 21) (1, 21)\n",
      "(107201, 21)\n",
      "(40200, 21) (13401, 21)\n",
      "(231, 21)\n",
      "(87, 21) (29, 21)\n",
      "(97277, 21)\n",
      "(36479, 21) (12160, 21)\n",
      "(3, 21)\n",
      "(1, 21) (1, 21)\n",
      "(4, 21)\n",
      "(1, 21) (1, 21)\n",
      "(264, 21)\n",
      "(99, 21) (33, 21)\n",
      "(1040, 21)\n",
      "(390, 21) (130, 21)\n",
      "(10, 21)\n",
      "(3, 21) (2, 21)\n",
      "(1589, 21)\n",
      "(596, 21) (199, 21)\n",
      "(280790, 21)\n",
      "(105296, 21) (35099, 21)\n",
      "(2, 21)\n",
      "(0, 21) (1, 21)\n",
      "(979, 21)\n",
      "(367, 21) (123, 21)\n",
      "(1020, 21)\n",
      "(382, 21) (128, 21)\n",
      "(20, 21)\n",
      "(7, 21) (3, 21)\n",
      "46\n"
     ]
    }
   ],
   "source": [
    "### label encoding\n",
    "label_encoder = preprocessing.LabelEncoder() \n",
    "label_coding('protocol_type')\n",
    "label_coding('service')\n",
    "label_coding('flag')\n",
    "label_coding('label')\n",
    "\n",
    "### extracting features\n",
    "X=dataset.iloc[:,:-1]\n",
    "X=X.values\n",
    "scaler.fit(X)\n",
    "X=scaler.transform(X)\n",
    "\n",
    "### dimention reduction\n",
    "number_of_components=20\n",
    "pca = PCA(n_components=number_of_components)\n",
    "columns_array=[]\n",
    "for i in range (number_of_components):\n",
    "    columns_array.append(\"principal_Component\"+str(i+1))\n",
    "    \n",
    "principalComponents = pca.fit_transform(X)\n",
    "principalDf = pd.DataFrame(data = principalComponents\n",
    "              , columns = columns_array)\n",
    "\n",
    "finalDf = pd.concat([principalDf, dataset[['label']]], axis = 1)\n",
    "dataset=finalDf\n",
    "\n",
    "\n",
    "len_dataset= len(dataset.label.unique())\n",
    "list_train,list_test,partial_list=[],[],[]\n",
    "    \n",
    "crop_dataset(len_dataset)\n",
    "\n",
    "#print(list_train[0].shape[0])\n",
    "list_train.sort(key=sortingFunction,reverse=True)\n",
    "list_test.sort(key=sortingFunction,reverse=True)\n",
    "\n",
    "\n",
    "# to take top 15 features\n",
    "labels_to_consider = 15\n",
    "max_samples = list_train[0].shape[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 266
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 782,
     "status": "ok",
     "timestamp": 1575153532923,
     "user": {
      "displayName": "Nur Imtiazul Haque",
      "photoUrl": "",
      "userId": "15560043854638298091"
     },
     "user_tz": 300
    },
    "id": "KsD9UHC1c3me",
    "outputId": "4d335f10-82e7-4177-9514-214b30192660"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_sample: 40200 max_sample: 105296 need_to_create: 0\n",
      "n_sample: 36479 max_sample: 105296 need_to_create: 0\n",
      "n_sample: 826 max_sample: 105296 need_to_create: 0\n",
      "n_sample: 596 max_sample: 105296 need_to_create: 0\n",
      "n_sample: 468 max_sample: 105296 need_to_create: 32\n",
      "n_sample: 390 max_sample: 105296 need_to_create: 110\n",
      "n_sample: 382 max_sample: 105296 need_to_create: 118\n",
      "n_sample: 367 max_sample: 105296 need_to_create: 133\n",
      "n_sample: 99 max_sample: 105296 need_to_create: 401\n",
      "n_sample: 87 max_sample: 105296 need_to_create: 413\n",
      "n_sample: 20 max_sample: 105296 need_to_create: 480\n",
      "n_sample: 11 max_sample: 105296 need_to_create: 489\n",
      "n_sample: 8 max_sample: 105296 need_to_create: 492\n",
      "n_sample: 7 max_sample: 105296 need_to_create: 493\n",
      "        principal_Component1  principal_Component2  principal_Component3  \\\n",
      "487711             -2.347107             -0.237014             -0.806739   \n",
      "320639             -2.347107             -0.237014             -0.806739   \n",
      "275317             -2.347107             -0.237014             -0.806739   \n",
      "103133             -2.347107             -0.237014             -0.806739   \n",
      "47222              -2.347107             -0.237014             -0.806739   \n",
      "131096             -2.343037             -0.235676             -0.801211   \n",
      "307363             -2.347107             -0.237014             -0.806739   \n",
      "168820             -2.347107             -0.237014             -0.806739   \n",
      "297809             -2.347107             -0.237014             -0.806739   \n",
      "434637             -2.347108             -0.237014             -0.806740   \n",
      "134145             -2.347107             -0.237014             -0.806739   \n",
      "189923             -2.347107             -0.237014             -0.806739   \n",
      "322735             -2.347107             -0.237014             -0.806739   \n",
      "488496             -2.347107             -0.237014             -0.806739   \n",
      "328846             -2.347107             -0.237014             -0.806739   \n",
      "100340             -2.347107             -0.237014             -0.806739   \n",
      "160474             -2.347107             -0.237014             -0.806739   \n",
      "44288              -2.347107             -0.237014             -0.806739   \n",
      "266817             -2.347107             -0.237014             -0.806739   \n",
      "302853             -2.347107             -0.237014             -0.806739   \n",
      "487277             -2.345072             -0.236345             -0.803975   \n",
      "229957             -2.347107             -0.237014             -0.806739   \n",
      "306248             -2.347107             -0.237014             -0.806739   \n",
      "180709             -2.347107             -0.237014             -0.806739   \n",
      "410137             -2.220916             -0.195566             -0.635369   \n",
      "489930             -2.347107             -0.237014             -0.806739   \n",
      "191630             -2.347107             -0.237014             -0.806739   \n",
      "301491             -2.347107             -0.237014             -0.806739   \n",
      "201983             -2.347107             -0.237014             -0.806739   \n",
      "423002             -2.347108             -0.237014             -0.806740   \n",
      "...                      ...                   ...                   ...   \n",
      "53248               0.599075              0.324890              1.810169   \n",
      "76099              -1.142403              2.045597              4.893624   \n",
      "43148               4.857476             -2.254178             -0.002586   \n",
      "76071              -0.505067              2.050975              4.910790   \n",
      "76098              -1.099878              2.050464              4.924251   \n",
      "76127              -0.506319              2.042364              4.885252   \n",
      "43162               4.857476             -2.254178             -0.002586   \n",
      "75275              -0.744558              2.002021              4.827847   \n",
      "76137              -0.755496              2.000961              4.819435   \n",
      "43128               4.857476             -2.254178             -0.002586   \n",
      "43141               4.857476             -2.254178             -0.002586   \n",
      "75270              -0.626468              2.015674              4.829697   \n",
      "43104               4.857476             -2.254178             -0.002586   \n",
      "22782               4.474697              8.601241              0.085336   \n",
      "22783               4.471637              8.599179              0.079963   \n",
      "22786               4.212677              8.524096              0.297010   \n",
      "22751               4.530361              4.144164              1.087508   \n",
      "22761               4.514240              8.545209              0.194554   \n",
      "22789               4.216434              8.519725              0.292917   \n",
      "22781               4.477758              8.603303              0.090710   \n",
      "82522               1.237448              1.550237              9.320334   \n",
      "149012              0.123399              1.482077              8.355592   \n",
      "485175              1.367713              1.560507              9.425094   \n",
      "485179              1.556567              1.970616              9.293518   \n",
      "76493               3.788180              2.026326              9.421162   \n",
      "76192               2.950516             -0.005331              5.007135   \n",
      "76491               3.793751              2.025711              9.420119   \n",
      "77157               0.073468              1.725004              8.666971   \n",
      "77145              -0.017319              0.713004              2.414518   \n",
      "77159               0.067415              1.721238              8.658254   \n",
      "\n",
      "        principal_Component4  principal_Component5  principal_Component6  \\\n",
      "487711              0.114789              0.034166             -0.019161   \n",
      "320639              0.114789              0.034166             -0.019161   \n",
      "275317              0.114789              0.034166             -0.019161   \n",
      "103133              0.114789              0.034166             -0.019161   \n",
      "47222               0.114789              0.034166             -0.019161   \n",
      "131096              0.113885              0.033793             -0.018676   \n",
      "307363              0.114789              0.034166             -0.019161   \n",
      "168820              0.114789              0.034166             -0.019161   \n",
      "297809              0.114789              0.034166             -0.019161   \n",
      "434637              0.114789              0.034164             -0.019164   \n",
      "134145              0.114789              0.034166             -0.019161   \n",
      "189923              0.114789              0.034166             -0.019161   \n",
      "322735              0.114789              0.034166             -0.019161   \n",
      "488496              0.114789              0.034166             -0.019161   \n",
      "328846              0.114789              0.034166             -0.019161   \n",
      "100340              0.114789              0.034166             -0.019161   \n",
      "160474              0.114789              0.034166             -0.019161   \n",
      "44288               0.114789              0.034166             -0.019161   \n",
      "266817              0.114789              0.034166             -0.019161   \n",
      "302853              0.114789              0.034166             -0.019161   \n",
      "487277              0.114337              0.033980             -0.018919   \n",
      "229957              0.114789              0.034166             -0.019161   \n",
      "306248              0.114789              0.034166             -0.019161   \n",
      "180709              0.114789              0.034166             -0.019161   \n",
      "410137              0.086782              0.022601             -0.004118   \n",
      "489930              0.114789              0.034166             -0.019161   \n",
      "191630              0.114789              0.034166             -0.019161   \n",
      "301491              0.114789              0.034166             -0.019161   \n",
      "201983              0.114789              0.034166             -0.019161   \n",
      "423002              0.114789              0.034164             -0.019164   \n",
      "...                      ...                   ...                   ...   \n",
      "53248              -0.294588             -0.115228              0.751067   \n",
      "76099              -0.929448             -1.687203             -1.415392   \n",
      "43148               0.115769              0.739673              5.306210   \n",
      "76071              -0.925124             -1.647990             -1.342402   \n",
      "76098              -0.934455             -1.687575             -1.409861   \n",
      "76127              -0.920590             -1.642583             -1.337458   \n",
      "43162               0.115769              0.739673              5.306210   \n",
      "75275              -0.911591             -1.625893             -1.324729   \n",
      "76137              -0.910185             -1.625826             -1.326442   \n",
      "43128               0.115769              0.739673              5.306210   \n",
      "43141               0.115769              0.739673              5.306210   \n",
      "75270              -0.911162             -1.629506             -1.328560   \n",
      "43104               0.115769              0.739673              5.306210   \n",
      "22782               2.289096              0.531256             -2.638059   \n",
      "22783               2.289982              0.532217             -2.637136   \n",
      "22786               2.267732              1.430158             -2.674594   \n",
      "22751               2.193384              0.421696             -2.452827   \n",
      "22761               2.272847              0.511438             -2.644525   \n",
      "22789               2.268285              1.431897             -2.676067   \n",
      "22781               2.288210              0.530294             -2.638982   \n",
      "82522              23.558459              3.760504             -0.675081   \n",
      "149012             22.080758              1.684779             -1.110334   \n",
      "485175             23.539538              3.751019             -0.726170   \n",
      "485179             23.552028              3.749235             -0.817539   \n",
      "76493              -2.005058             -6.464209             -9.121200   \n",
      "76192              -1.108393             -4.112649             -6.486580   \n",
      "76491              -2.004784             -6.463338             -9.119945   \n",
      "77157               4.221599             -1.056723             -0.184914   \n",
      "77145              -0.409225             -0.314794              0.141881   \n",
      "77159               4.224899             -1.054905             -0.182401   \n",
      "\n",
      "        principal_Component7  principal_Component8  principal_Component9  \\\n",
      "487711              0.022585             -0.004450              0.081642   \n",
      "320639              0.022585             -0.004450              0.081642   \n",
      "275317              0.022585             -0.004450              0.081642   \n",
      "103133              0.022585             -0.004450              0.081642   \n",
      "47222               0.022585             -0.004450              0.081642   \n",
      "131096              0.022198             -0.004345              0.080165   \n",
      "307363              0.022585             -0.004450              0.081642   \n",
      "168820              0.022585             -0.004450              0.081642   \n",
      "297809              0.022585             -0.004450              0.081642   \n",
      "434637              0.022585             -0.004450              0.081635   \n",
      "134145              0.022585             -0.004450              0.081642   \n",
      "189923              0.022585             -0.004450              0.081642   \n",
      "322735              0.022585             -0.004450              0.081642   \n",
      "488496              0.022585             -0.004450              0.081642   \n",
      "328846              0.022585             -0.004450              0.081642   \n",
      "100340              0.022585             -0.004450              0.081642   \n",
      "160474              0.022585             -0.004450              0.081642   \n",
      "44288               0.022585             -0.004450              0.081642   \n",
      "266817              0.022585             -0.004450              0.081642   \n",
      "302853              0.022585             -0.004450              0.081642   \n",
      "487277              0.022392             -0.004397              0.080904   \n",
      "229957              0.022585             -0.004450              0.081642   \n",
      "306248              0.022585             -0.004450              0.081642   \n",
      "180709              0.022585             -0.004450              0.081642   \n",
      "410137              0.010593             -0.001188              0.035820   \n",
      "489930              0.022585             -0.004450              0.081642   \n",
      "191630              0.022585             -0.004450              0.081642   \n",
      "301491              0.022585             -0.004450              0.081642   \n",
      "201983              0.022585             -0.004450              0.081642   \n",
      "423002              0.022585             -0.004450              0.081635   \n",
      "...                      ...                   ...                   ...   \n",
      "53248              -0.185613              0.087708             -0.817870   \n",
      "76099               0.502026             -0.252905              5.936671   \n",
      "43148              -0.085279              0.237171              1.495926   \n",
      "76071               0.582458             -0.268443              6.346841   \n",
      "76098               0.497536             -0.252242              5.918117   \n",
      "76127               0.582948             -0.268705              6.346472   \n",
      "43162              -0.085279              0.237171              1.495926   \n",
      "75275               0.525687             -0.255008              6.003597   \n",
      "76137               0.527560             -0.255241              6.011328   \n",
      "43128              -0.085279              0.237171              1.495926   \n",
      "43141              -0.085279              0.237171              1.495926   \n",
      "75270               0.559678             -0.262519              6.194922   \n",
      "43104              -0.085279              0.237171              1.495926   \n",
      "22782              28.220118             35.109426             -0.880299   \n",
      "22783              28.219308             35.109441             -0.884019   \n",
      "22786              28.203320             35.126110             -0.859233   \n",
      "22751              28.357116             35.133601             -0.068200   \n",
      "22761              28.242688             35.109183             -0.770840   \n",
      "22789              28.199265             35.126402             -0.878887   \n",
      "22781              28.220928             35.109411             -0.876580   \n",
      "82522              42.310117            -27.984245             -5.202072   \n",
      "149012             38.665406            -24.523030             -4.741197   \n",
      "485175             42.288875            -27.984395             -5.304792   \n",
      "485179             42.278066            -27.990164             -5.388909   \n",
      "76493               8.726956             -3.824421             95.558679   \n",
      "76192               7.391755             -3.277452             83.789013   \n",
      "76491               8.727359             -3.824457             95.559905   \n",
      "77157              27.399922             36.270991            -12.086686   \n",
      "77145               0.064268              0.051676              0.374100   \n",
      "77159              27.406957             36.282402            -12.097837   \n",
      "\n",
      "        principal_Component10  ...  principal_Component12  \\\n",
      "487711              -0.000965  ...              -0.003491   \n",
      "320639              -0.000965  ...              -0.003491   \n",
      "275317              -0.000965  ...              -0.003491   \n",
      "103133              -0.000965  ...              -0.003491   \n",
      "47222               -0.000965  ...              -0.003491   \n",
      "131096              -0.000913  ...              -0.003400   \n",
      "307363              -0.000965  ...              -0.003491   \n",
      "168820              -0.000965  ...              -0.003491   \n",
      "297809              -0.000965  ...              -0.003491   \n",
      "434637              -0.001008  ...              -0.003515   \n",
      "134145              -0.000965  ...              -0.003491   \n",
      "189923              -0.000965  ...              -0.003491   \n",
      "322735              -0.000965  ...              -0.003491   \n",
      "488496              -0.000965  ...              -0.003491   \n",
      "328846              -0.000965  ...              -0.003491   \n",
      "100340              -0.000965  ...              -0.003491   \n",
      "160474              -0.000965  ...              -0.003491   \n",
      "44288               -0.000965  ...              -0.003491   \n",
      "266817              -0.000965  ...              -0.003491   \n",
      "302853              -0.000965  ...              -0.003491   \n",
      "487277              -0.000939  ...              -0.003445   \n",
      "229957              -0.000965  ...              -0.003491   \n",
      "306248              -0.000965  ...              -0.003491   \n",
      "180709              -0.000965  ...              -0.003491   \n",
      "410137               0.000620  ...              -0.000698   \n",
      "489930              -0.000965  ...              -0.003491   \n",
      "191630              -0.000965  ...              -0.003491   \n",
      "301491              -0.000965  ...              -0.003491   \n",
      "201983              -0.000965  ...              -0.003491   \n",
      "423002              -0.001008  ...              -0.003515   \n",
      "...                       ...  ...                    ...   \n",
      "53248               -0.262777  ...               0.046081   \n",
      "76099               -0.097546  ...               0.242057   \n",
      "43148                0.038004  ...              -0.230099   \n",
      "76071               -0.199398  ...               0.213789   \n",
      "76098               -0.097189  ...               0.243020   \n",
      "76127               -0.202062  ...               0.213593   \n",
      "43162                0.038004  ...              -0.230099   \n",
      "75275               -0.144030  ...               0.227889   \n",
      "76137               -0.144319  ...               0.227393   \n",
      "43128                0.038004  ...              -0.230099   \n",
      "43141                0.038004  ...              -0.230099   \n",
      "75270               -0.176823  ...               0.218867   \n",
      "43104                0.038004  ...              -0.230099   \n",
      "22782               -1.273030  ...               3.701734   \n",
      "22783               -1.273166  ...               3.702048   \n",
      "22786               -1.310514  ...               3.693591   \n",
      "22751               -1.240400  ...               3.660874   \n",
      "22761               -1.268520  ...               3.693090   \n",
      "22789               -1.310423  ...               3.695318   \n",
      "22781               -1.272894  ...               3.701421   \n",
      "82522               -4.828928  ...              -7.008845   \n",
      "149012              -5.909783  ...              -9.461665   \n",
      "485175              -4.823319  ...              -6.989629   \n",
      "485179              -4.826046  ...              -6.980801   \n",
      "76493              -32.693715  ...             -13.347717   \n",
      "76192              -31.823075  ...             -13.512745   \n",
      "76491              -32.694393  ...             -13.347940   \n",
      "77157               12.422568  ...            -132.761106   \n",
      "77145               -0.058601  ...              -0.068129   \n",
      "77159               12.426433  ...            -132.802222   \n",
      "\n",
      "        principal_Component13  principal_Component14  principal_Component15  \\\n",
      "487711              -0.001190              -0.040232               0.024044   \n",
      "320639              -0.001190              -0.040232               0.024044   \n",
      "275317              -0.001190              -0.040232               0.024044   \n",
      "103133              -0.001190              -0.040232               0.024044   \n",
      "47222               -0.001190              -0.040232               0.024044   \n",
      "131096              -0.001124              -0.039347               0.023266   \n",
      "307363              -0.001190              -0.040232               0.024044   \n",
      "168820              -0.001190              -0.040232               0.024044   \n",
      "297809              -0.001190              -0.040232               0.024044   \n",
      "434637              -0.001205              -0.040238               0.024047   \n",
      "134145              -0.001190              -0.040232               0.024044   \n",
      "189923              -0.001190              -0.040232               0.024044   \n",
      "322735              -0.001190              -0.040232               0.024044   \n",
      "488496              -0.001190              -0.040232               0.024044   \n",
      "328846              -0.001190              -0.040232               0.024044   \n",
      "100340              -0.001190              -0.040232               0.024044   \n",
      "160474              -0.001190              -0.040232               0.024044   \n",
      "44288               -0.001190              -0.040232               0.024044   \n",
      "266817              -0.001190              -0.040232               0.024044   \n",
      "302853              -0.001190              -0.040232               0.024044   \n",
      "487277              -0.001157              -0.039790               0.023655   \n",
      "229957              -0.001190              -0.040232               0.024044   \n",
      "306248              -0.001190              -0.040232               0.024044   \n",
      "180709              -0.001190              -0.040232               0.024044   \n",
      "410137               0.000867              -0.012805              -0.000056   \n",
      "489930              -0.001190              -0.040232               0.024044   \n",
      "191630              -0.001190              -0.040232               0.024044   \n",
      "301491              -0.001190              -0.040232               0.024044   \n",
      "201983              -0.001190              -0.040232               0.024044   \n",
      "423002              -0.001205              -0.040238               0.024047   \n",
      "...                       ...                    ...                    ...   \n",
      "53248                0.081411               0.444412              -0.456682   \n",
      "76099               -0.558346              -3.839952               1.252803   \n",
      "43148               -0.834485              -0.625714               0.530866   \n",
      "76071               -0.548929              -4.122429               1.256695   \n",
      "76098               -0.557581              -3.827405               1.247483   \n",
      "76127               -0.547790              -4.121366               1.257106   \n",
      "43162               -0.834485              -0.625714               0.530866   \n",
      "75275               -0.546896              -3.891123               1.241565   \n",
      "76137               -0.547212              -3.896538               1.243348   \n",
      "43128               -0.834485              -0.625714               0.530866   \n",
      "43141               -0.834485              -0.625714               0.530866   \n",
      "75270               -0.547122              -4.020242               1.252542   \n",
      "43104               -0.834485              -0.625714               0.530866   \n",
      "22782               -4.083216              -2.497439             -31.402726   \n",
      "22783               -4.082882              -2.494478             -31.402990   \n",
      "22786               -4.116429              -2.495220             -31.370047   \n",
      "22751               -4.206881              -3.011343             -31.260179   \n",
      "22761               -4.093669              -2.578404             -31.392942   \n",
      "22789               -4.114976              -2.481157             -31.371880   \n",
      "22781               -4.083550              -2.500401             -31.402463   \n",
      "82522              -11.795438              -1.649138              -0.961082   \n",
      "149012             -19.135052              -1.185941              -1.727557   \n",
      "485175             -11.791949              -1.583988              -0.974527   \n",
      "485179             -11.762640              -1.536894              -1.009049   \n",
      "76493               16.323332             109.635816              -8.896108   \n",
      "76192               16.902084             117.190706             -10.650946   \n",
      "76491               16.323483             109.634889              -8.896224   \n",
      "77157               47.877615              -7.242263              35.032274   \n",
      "77145               -0.035222              -0.404414              -0.228942   \n",
      "77159               47.893597              -7.238617              35.042276   \n",
      "\n",
      "        principal_Component16  principal_Component17  principal_Component18  \\\n",
      "487711              -0.013476              -0.010291               0.038981   \n",
      "320639              -0.013476              -0.010291               0.038981   \n",
      "275317              -0.013476              -0.010291               0.038981   \n",
      "103133              -0.013476              -0.010291               0.038981   \n",
      "47222               -0.013476              -0.010291               0.038981   \n",
      "131096              -0.012985              -0.010042               0.037887   \n",
      "307363              -0.013476              -0.010291               0.038981   \n",
      "168820              -0.013476              -0.010291               0.038981   \n",
      "297809              -0.013476              -0.010291               0.038981   \n",
      "434637              -0.013478              -0.010290               0.038978   \n",
      "134145              -0.013476              -0.010291               0.038981   \n",
      "189923              -0.013476              -0.010291               0.038981   \n",
      "322735              -0.013476              -0.010291               0.038981   \n",
      "488496              -0.013476              -0.010291               0.038981   \n",
      "328846              -0.013476              -0.010291               0.038981   \n",
      "100340              -0.013476              -0.010291               0.038981   \n",
      "160474              -0.013476              -0.010291               0.038981   \n",
      "44288               -0.013476              -0.010291               0.038981   \n",
      "266817              -0.013476              -0.010291               0.038981   \n",
      "302853              -0.013476              -0.010291               0.038981   \n",
      "487277              -0.013230              -0.010166               0.038434   \n",
      "229957              -0.013476              -0.010291               0.038981   \n",
      "306248              -0.013476              -0.010291               0.038981   \n",
      "180709              -0.013476              -0.010291               0.038981   \n",
      "410137               0.001736              -0.002579               0.005062   \n",
      "489930              -0.013476              -0.010291               0.038981   \n",
      "191630              -0.013476              -0.010291               0.038981   \n",
      "301491              -0.013476              -0.010291               0.038981   \n",
      "201983              -0.013476              -0.010291               0.038981   \n",
      "423002              -0.013478              -0.010290               0.038978   \n",
      "...                       ...                    ...                    ...   \n",
      "53248                0.266804               0.122358              -0.194399   \n",
      "76099               -0.298462              -0.697073               3.174402   \n",
      "43148               -0.256102              -0.120608              -0.305811   \n",
      "76071               -0.220217              -0.745931               3.075154   \n",
      "76098               -0.295925              -0.695320               3.171951   \n",
      "76127               -0.220120              -0.747779               3.085640   \n",
      "43162               -0.256102              -0.120608              -0.305811   \n",
      "75275               -0.266399              -0.721522               3.219991   \n",
      "76137               -0.266979              -0.722191               3.219904   \n",
      "43128               -0.256102              -0.120608              -0.305811   \n",
      "43141               -0.256102              -0.120608              -0.305811   \n",
      "75270               -0.242667              -0.737818               3.157442   \n",
      "43104               -0.256102              -0.120608              -0.305811   \n",
      "22782              -29.555002              -9.853016               0.321927   \n",
      "22783              -29.555277              -9.853078               0.325009   \n",
      "22786              -29.593910              -9.848284               0.361648   \n",
      "22751              -29.632998              -9.894967               0.102095   \n",
      "22761              -29.549519              -9.855076               0.257818   \n",
      "22789              -29.595337              -9.847322               0.369419   \n",
      "22781              -29.554727              -9.852954               0.318846   \n",
      "82522               -1.646466              65.381492               8.049869   \n",
      "149012               0.943325              64.888053               8.064827   \n",
      "485175              -1.656474              65.394850               8.044536   \n",
      "485179              -1.627547              65.398840               8.062617   \n",
      "76493               -1.060722               2.468865              -2.032525   \n",
      "76192               -1.268560               3.561975              -3.096131   \n",
      "76491               -1.060265               2.468496              -2.031928   \n",
      "77157               17.820579             -13.317879               5.371317   \n",
      "77145                0.255838               0.099121              -1.005334   \n",
      "77159               17.826150             -13.322289               5.379602   \n",
      "\n",
      "        principal_Component19  principal_Component20  label  \n",
      "487711              -0.002493               0.024827     18  \n",
      "320639              -0.002493               0.024827     18  \n",
      "275317              -0.002493               0.024827     18  \n",
      "103133              -0.002493               0.024827     18  \n",
      "47222               -0.002493               0.024827     18  \n",
      "131096              -0.002472               0.023235     18  \n",
      "307363              -0.002493               0.024827     18  \n",
      "168820              -0.002493               0.024827     18  \n",
      "297809              -0.002493               0.024827     18  \n",
      "434637              -0.002492               0.024831     18  \n",
      "134145              -0.002493               0.024827     18  \n",
      "189923              -0.002493               0.024827     18  \n",
      "322735              -0.002493               0.024827     18  \n",
      "488496              -0.002493               0.024827     18  \n",
      "328846              -0.002493               0.024827     18  \n",
      "100340              -0.002493               0.024827     18  \n",
      "160474              -0.002493               0.024827     18  \n",
      "44288               -0.002493               0.024827     18  \n",
      "266817              -0.002493               0.024827     18  \n",
      "302853              -0.002493               0.024827     18  \n",
      "487277              -0.002482               0.024031     18  \n",
      "229957              -0.002493               0.024827     18  \n",
      "306248              -0.002493               0.024827     18  \n",
      "180709              -0.002493               0.024827     18  \n",
      "410137              -0.001840              -0.024522     18  \n",
      "489930              -0.002493               0.024827     18  \n",
      "191630              -0.002493               0.024827     18  \n",
      "301491              -0.002493               0.024827     18  \n",
      "201983              -0.002493               0.024827     18  \n",
      "423002              -0.002492               0.024831     18  \n",
      "...                       ...                    ...    ...  \n",
      "53248                0.005299              -1.429726     10  \n",
      "76099               -0.065379              -0.076998     10  \n",
      "43148               -0.310398              -4.837589     10  \n",
      "76071               -0.125401              -0.368072     10  \n",
      "76098               -0.065365              -0.097214     10  \n",
      "76127               -0.126603              -0.378875     10  \n",
      "43162               -0.310398              -4.837589     10  \n",
      "75275               -0.084295              -0.216056     10  \n",
      "76137               -0.084346              -0.208602     10  \n",
      "43128               -0.310398              -4.837589     10  \n",
      "43141               -0.310398              -4.837589     10  \n",
      "75270               -0.106873              -0.291910     10  \n",
      "43104               -0.310398              -4.837589     10  \n",
      "22782               -4.228383               0.453158      3  \n",
      "22783               -4.228502               0.448998      3  \n",
      "22786               -4.230988               0.331422      3  \n",
      "22751               -4.259630               0.668634      3  \n",
      "22761               -4.228645               0.539899      3  \n",
      "22789               -4.230368               0.320243      3  \n",
      "22781               -4.228264               0.457317      3  \n",
      "82522              -34.394206               0.444182      1  \n",
      "149012             -37.789381               1.195278      1  \n",
      "485175             -34.382598               0.464788      1  \n",
      "485179             -34.381322               0.499486      1  \n",
      "76493                0.809478              -1.478004      6  \n",
      "76192                1.630830               1.347888      6  \n",
      "76491                0.809205              -1.479761      6  \n",
      "77157               -3.047071              -0.991048     22  \n",
      "77145                0.034447               0.188381     22  \n",
      "77159               -3.048392              -0.999064     22  \n",
      "\n",
      "[246987 rows x 21 columns]\n"
     ]
    }
   ],
   "source": [
    "train_dataframe=list_train[0]\n",
    "test_dataframe=list_test[0]\n",
    "\n",
    "for i in range(1,labels_to_consider):\n",
    "    \n",
    "    ##########################################\n",
    "    new_sample_generation(len(list_train[i]),max_samples,int((max_samples-len(list_train[i]))*.5))\n",
    "    #########################################\n",
    "    \n",
    "    train_dataframe=pd.concat([train_dataframe,list_train[i]])\n",
    "    test_dataframe=pd.concat([test_dataframe,list_test[i]])\n",
    "\n",
    "partial_dataframe=pd.concat([train_dataframe,test_dataframe])\n",
    "print(partial_dataframe)\n",
    "#print(len(test_dataframe.label.unique()))\n",
    "\n",
    "x_train=np.array(train_dataframe.iloc[:,:-1])\n",
    "x_test=np.array(test_dataframe.iloc[:,:-1])\n",
    "y_train=np.array(train_dataframe.iloc[:,-1])\n",
    "y_test=np.array(test_dataframe.iloc[:,-1])   \n",
    "\n",
    "\n",
    "\n",
    "def to_GAN(x_g, y_g,numOfSamples):\n",
    "    #print(\"Add GAN\")\n",
    "    return np.random.randn(numOfSamples,x_g.shape[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(246987, 21)"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "WavlGL9pc3mp"
   },
   "outputs": [],
   "source": [
    "numOfSamples=5\n",
    "flag=1\n",
    "\n",
    "for i in np.unique(y_train):\n",
    "    if (i!=14):\n",
    "        continue\n",
    "    print(\"Calling 14 only\")\n",
    "    x_g=np.copy(x_train)\n",
    "    \n",
    "    y_g=np.zeros(y_train.shape)\n",
    "    \n",
    "    y_g[y_train==i]=1\n",
    "    \n",
    "    yganlist=y_g.tolist()\n",
    "    \n",
    "    print(yganlist.count(1), yganlist.count(0))\n",
    "    \n",
    "    x_syn=to_GAN(x_g, y_g,numOfSamples)\n",
    "    y_syn=np.ones(numOfSamples)*i\n",
    "    if flag==1:\n",
    "        X_Gan=x_syn\n",
    "        y_Gan=y_syn\n",
    "        flag=0\n",
    "        #print(\"First\")\n",
    "    else:\n",
    "        X_Gan=np.concatenate((X_Gan,x_syn),axis=0)\n",
    "        y_Gan=np.concatenate((y_Gan,y_syn))        \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "yganlist=y_g.tolist()\n",
    "print(x_g.shape)\n",
    "print(yganlist.count(1), yganlist.count(0),yganlist.count(1) + yganlist.count(0) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "oF1vR6pZc3mx"
   },
   "outputs": [],
   "source": [
    "from sklearn.datasets import make_circles, make_moons\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "\n",
    "scaler = StandardScaler()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib\n",
    "import numpy as np\n",
    "\n",
    "import random\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "import matplotlib.pyplot as plt\n",
    "from torch import Tensor\n",
    "\n",
    "from torch import nn, optim\n",
    "from torch.autograd.variable import Variable\n",
    "from torchvision import transforms, datasets\n",
    "import torch.nn.functional as Func\n",
    "import sys\n",
    "\n",
    "data = [(x, y) for x, y in zip(x_g,y_g)]\n",
    "\n",
    "#data = [[0, 0], [0, 0], [1, 1], [1, 1]]\n",
    "#print(scaler.fit(data))\n",
    "\n",
    "class Data_Loader(): \n",
    "    def __init__(self,data_list):       \n",
    "        self.data=data_list\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        img = self.data[index][0]\n",
    "        #img=img/np.absolute(img).max()\n",
    "        #img = img.reshape(28, 28) / 255.0\n",
    "        img_tensor = Tensor(img).float()\n",
    "        label = self.data[index][1]\n",
    "        #label=torch.from_numpy(label).type(torch.LongTensor)\n",
    "        #label=Tensor(label).long()\n",
    "        return (img_tensor, label)\n",
    "\n",
    "dataSet=Data_Loader(data)\n",
    "data_loader = DataLoader(dataset=dataSet, batch_size=1000, shuffle=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "for d in data_loader:\n",
    "    print(d[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "g-Anl0yic3m3"
   },
   "outputs": [],
   "source": [
    "class DiscriminatorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer discriminative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(DiscriminatorNet, self).__init__()\n",
    "        n_features = 20\n",
    "        n_out = 1\n",
    "        \n",
    "        \n",
    "        self.hidden0 = nn.Sequential( \n",
    "            nn.Linear(n_features, 50),\n",
    "            nn.Tanh()\n",
    "            #nn.Dropout(0.3)\n",
    "        )\n",
    "        '''\n",
    "        self.hidden1 = nn.Sequential(\n",
    "            nn.Linear(4, 6),\n",
    "            nn.Tanh()\n",
    "            #nn.Dropout(0.3)\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(6, 3),\n",
    "            nn.Tanh()\n",
    "            #nn.Dropout(0.3)\n",
    "        )\n",
    "        '''\n",
    "        self.out = nn.Sequential(\n",
    "            torch.nn.Linear(50, n_out),\n",
    "            torch.nn.Sigmoid()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        #print(\"Discriminator\", x)\n",
    "        #x = self.hidden1(x)\n",
    "        #x = self.hidden2(x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "discriminator = DiscriminatorNet()\n",
    "\n",
    "\n",
    "class GeneratorNet(torch.nn.Module):\n",
    "    \"\"\"\n",
    "    A three hidden-layer generative neural network\n",
    "    \"\"\"\n",
    "    def __init__(self):\n",
    "        super(GeneratorNet, self).__init__()\n",
    "        n_noise = 10\n",
    "        n_out = 20\n",
    "        \n",
    "        self.hidden0 = nn.Sequential(\n",
    "            nn.Linear(n_noise, 50),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        '''\n",
    "        self.hidden1 = nn.Sequential(            \n",
    "            nn.Linear(4, 8),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        self.hidden2 = nn.Sequential(\n",
    "            nn.Linear(8, 5),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "        '''\n",
    "        self.out = nn.Sequential(\n",
    "            nn.Linear(50, n_out),\n",
    "            nn.Tanh()\n",
    "        )\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.hidden0(x)\n",
    "        #x = self.hidden1(x)\n",
    "        #x = self.hidden2(x)\n",
    "        #print(\"Generator\", x)\n",
    "        x = self.out(x)\n",
    "        return x\n",
    "generator = GeneratorNet()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "rdpjAyysc3m7"
   },
   "outputs": [],
   "source": [
    "def noise(size):\n",
    "    '''\n",
    "    Generates a 1-d vector of gaussian sampled random values\n",
    "    '''\n",
    "    n = Variable(torch.randn(size, 10))\n",
    "    return n\n",
    "\n",
    "def ones_target(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.ones(size, 1))\n",
    "    return data\n",
    "\n",
    "def zeros_target(size):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    data = Variable(torch.zeros(size, 1))\n",
    "    \n",
    "    return data\n",
    "\n",
    "def true_target(y):\n",
    "    '''\n",
    "    Tensor containing zeros, with shape = size\n",
    "    '''\n",
    "    #data = Variable(torch.zeros(size, 1))\n",
    "    data= Variable(torch.from_numpy(y).type(torch.FloatTensor))\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4VADZW0vc3m_"
   },
   "outputs": [],
   "source": [
    "def train_discriminator(optimizer, real_data, fake_data, y_real):\n",
    "    \n",
    "    N = real_data.size(0)\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # 1.1 Train on Real Data\n",
    "    prediction_real = discriminator(real_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_real = loss(prediction_real, true_target(y_real))\n",
    "    error_real.backward()\n",
    "    \n",
    "    # 1.2 Train on Fake Data\n",
    "    prediction_fake = discriminator(fake_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error_fake = loss(prediction_fake, zeros_target(N))\n",
    "    error_fake.backward()\n",
    "    \n",
    "    # 1.3 Update weights with gradients\n",
    "    optimizer.step()\n",
    "    \n",
    "    # Return error and predictions for real and fake inputs\n",
    "    return error_real + error_fake, prediction_real, prediction_fake\n",
    "\n",
    "\n",
    "def train_generator(optimizer, fake_data):\n",
    "    N = fake_data.size(0)\n",
    "    # Reset gradients\n",
    "    optimizer.zero_grad()\n",
    "    # Sample noise and generate fake data\n",
    "    prediction = discriminator(fake_data)\n",
    "    # Calculate error and backpropagate\n",
    "    error = loss(prediction, ones_target(N))\n",
    "    error.backward()\n",
    "    # Update weights with gradients\n",
    "    optimizer.step()\n",
    "    # Return error\n",
    "    return error\n",
    "\n",
    "\n",
    "###########################################################\n",
    "d_optimizer = optim.Adam(discriminator.parameters(), lr=0.0002)\n",
    "g_optimizer = optim.Adam(generator.parameters(), lr=0.0002)\n",
    "\n",
    "loss = nn.BCELoss()\n",
    "###########################################################\n",
    "num_test_samples = 20\n",
    "test_noise = noise(num_test_samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 381
    },
    "colab_type": "code",
    "executionInfo": {
     "elapsed": 382,
     "status": "error",
     "timestamp": 1575153675237,
     "user": {
      "displayName": "Nur Imtiazul Haque",
      "photoUrl": "",
      "userId": "15560043854638298091"
     },
     "user_tz": 300
    },
    "id": "jh1sEcPhc3nE",
    "outputId": "fc7b6580-fc6a-4c97-b70d-b5185a8c3864"
   },
   "outputs": [],
   "source": [
    "num_epochs = 2500\n",
    "d_errs=[]\n",
    "g_errs=[]\n",
    "\n",
    "print(data_loader)\n",
    "for epoch in range(num_epochs):\n",
    "    g_error_sum=0\n",
    "    d_error_sum=0\n",
    "    for n_batch, (real_batch,y_real) in enumerate(data_loader):\n",
    "        N = real_batch.size(0)\n",
    "                \n",
    "        # 1. Train Discriminator\n",
    "        real_data = Variable(real_batch)\n",
    "        #real_target=Variable(real_target)\n",
    "        # Generate fake data and detach \n",
    "        \n",
    "        #print(real_data.shape)\n",
    "        \n",
    "        # (so gradients are not calculated for generator)\n",
    "        fake_data = generator(noise(N)).detach()\n",
    "        \n",
    "        # Train D\n",
    "        y_real=np.array(y_real)\n",
    "        d_error, d_pred_real, d_pred_fake = train_discriminator(d_optimizer, real_data, fake_data,y_real)\n",
    "\n",
    "        # 2. Train Generator\n",
    "        \n",
    "        # Generate fake data\n",
    "        fake_data = generator(noise(N))\n",
    "        # Train G\n",
    "        g_error = train_generator(g_optimizer, fake_data)\n",
    "        \n",
    "        g_error_sum+=g_error\n",
    "        \n",
    "        d_error_sum+=d_error\n",
    "        \n",
    "        \n",
    "\n",
    "        \n",
    "        # Log batch error\n",
    "        #logger.log(d_error, g_error, epoch, n_batch, num_batches)\n",
    "        \n",
    "        # Display Progress every few batches\n",
    "        \n",
    "        #if (epoch) %200==0:\n",
    "        #    print(d_error, g_error, epoch, n_batch)\n",
    "    \n",
    "    d_errs.append(d_error_sum)\n",
    "    g_errs.append(g_error_sum)  \n",
    "    \n",
    "    if (epoch) % 50 == 0:\n",
    "        print(\"epoch: \",epoch)\n",
    "        test_noise = noise(num_test_samples)\n",
    "        test_images =(generator(test_noise))\n",
    "        test_images = test_images.data\n",
    "        real_data=real_data.data\n",
    "\n",
    "\n",
    "        plt.plot()\n",
    "        plt.scatter(real_data[:,0][y_real==1], real_data[:,1][y_real==1], s=40, marker='x',c='red')\n",
    "        plt.scatter(real_data[:,0][y_real==0], real_data[:,1][y_real==0], s=40, marker='o',c='blue')\n",
    "        plt.scatter(test_images[:,0], test_images[:,1], s=40, marker='p',c='green')\n",
    "        plt.axis('equal')\n",
    "        plt.show()\n",
    "\n",
    "        plt.plot()\n",
    "        plt.plot(d_errs)\n",
    "        plt.plot(g_errs)\n",
    "        plt.show()\n",
    "\n",
    "        if g_error>d_error:\n",
    "            pass\n",
    "        \n",
    "            #sys.exit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot()\n",
    "plt.scatter(real_data[:,0][y_real==1], real_data[:,1][y_real==1], s=40, marker='x',c='red')\n",
    "plt.scatter(real_data[:,0][y_real==0], real_data[:,1][y_real==0], s=40, marker='o',c='blue')\n",
    "plt.scatter(test_images[:,0], test_images[:,1], s=40, marker='p',c='green')\n",
    "plt.axis('equal')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "temp_dataframe_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "name": "11_30_2019.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
